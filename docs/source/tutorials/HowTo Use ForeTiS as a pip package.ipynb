{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HowTo: Use ForeTiS as a pip package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, we show how you can use ForeTiS as a pip package and also guide you through the steps that ForeTiS is doing when triggering an optimization run.\n",
    "\n",
    "Please clone the whole GitHub repository if you want to run this tutorial on your own, as we need the tutorial data from our GitHub repository and to make sure that all paths we define are correct: ``git clone https://github.com/grimmlab/ForeTiS.git``\n",
    "\n",
    "Then, start a Jupyter notebook server on your machine and open this Jupyter notebook, which is placed at ``docs/source/tutorials`` in the repository.\n",
    "\n",
    "However, you could also download the single files and define the paths yourself:\n",
    "\n",
    "- The Jupyter notebook can be downloaded here: [HowTo: Use ForeTiS as a pip package.ipynb](https://github.com/grimmlab/ForeTiS/tree/main/docs/source/tutorials/HowTo%20Use%20ForeTiS%20as%20a%20pip%20package.ipynb)\n",
    "\n",
    "- The data we use can be found here: [tutorial data](https://github.com/grimmlab/ForeTiS/tree/main/docs/source/tutorials/tutorial_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation, imports and paths\n",
    "First, we may need to install ForeTiS (uncomment if it is not already installed). Then, we import ForeTiS as well as further libraries that we need in this tutorial. In the end, we define some paths and filenames which we will use more often throughout this tutorial. We will save the results in the same directory where this repository is placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://test.pypi.org/simple/\n",
      "Requirement already satisfied: ForeTiS in /home/josef/.local/lib/python3.10/site-packages (0.0.1)\n",
      "Requirement already satisfied: statsmodels==0.13.2 in /home/josef/.local/lib/python3.10/site-packages (from ForeTiS) (0.13.2)\n",
      "Requirement already satisfied: xgboost==1.6.2 in /home/josef/.local/lib/python3.10/site-packages (from ForeTiS) (1.6.2)\n",
      "Requirement already satisfied: torch==1.12.1 in /home/josef/.local/lib/python3.10/site-packages (from ForeTiS) (1.12.1)\n",
      "Requirement already satisfied: scipy==1.8.1 in /home/josef/.local/lib/python3.10/site-packages (from ForeTiS) (1.8.1)\n",
      "Requirement already satisfied: optuna==3.0.2 in /home/josef/.local/lib/python3.10/site-packages (from ForeTiS) (3.0.2)\n",
      "Requirement already satisfied: blitz-bayesian-pytorch in /home/josef/.local/lib/python3.10/site-packages (from ForeTiS) (0.2.8)\n",
      "Requirement already satisfied: changefinder in /home/josef/.local/lib/python3.10/site-packages (from ForeTiS) (0.3)\n",
      "Requirement already satisfied: tensorflow-probability==0.18 in /home/josef/.local/lib/python3.10/site-packages (from ForeTiS) (0.18.0)\n",
      "Requirement already satisfied: scikit-learn==1.1.2 in /home/josef/.local/lib/python3.10/site-packages (from ForeTiS) (1.1.2)\n",
      "Requirement already satisfied: gpflow==2.5.2 in /home/josef/.local/lib/python3.10/site-packages (from ForeTiS) (2.5.2)\n",
      "Requirement already satisfied: tensorflow==2.10.0 in /home/josef/.local/lib/python3.10/site-packages (from ForeTiS) (2.10.0)\n",
      "Requirement already satisfied: joblib==1.2.0 in /home/josef/.local/lib/python3.10/site-packages (from ForeTiS) (1.2.0)\n",
      "Requirement already satisfied: bayesian-torch in /home/josef/.local/lib/python3.10/site-packages (from ForeTiS) (0.2.1)\n",
      "Requirement already satisfied: matplotlib in /home/josef/.local/lib/python3.10/site-packages (from ForeTiS) (3.3.3)\n",
      "Requirement already satisfied: pmdarima==2.0.1 in /home/josef/.local/lib/python3.10/site-packages (from ForeTiS) (2.0.1)\n",
      "Requirement already satisfied: pandas==1.5.0 in /home/josef/.local/lib/python3.10/site-packages (from ForeTiS) (1.5.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from gpflow==2.5.2->ForeTiS) (59.6.0)\n",
      "Requirement already satisfied: typing-extensions in /home/josef/.local/lib/python3.10/site-packages (from gpflow==2.5.2->ForeTiS) (4.3.0)\n",
      "Requirement already satisfied: packaging in /home/josef/.local/lib/python3.10/site-packages (from gpflow==2.5.2->ForeTiS) (21.3)\n",
      "Requirement already satisfied: tabulate in /home/josef/.local/lib/python3.10/site-packages (from gpflow==2.5.2->ForeTiS) (0.8.10)\n",
      "Requirement already satisfied: lark>=1.1.0 in /home/josef/.local/lib/python3.10/site-packages (from gpflow==2.5.2->ForeTiS) (1.1.2)\n",
      "Requirement already satisfied: multipledispatch>=0.6 in /home/josef/.local/lib/python3.10/site-packages (from gpflow==2.5.2->ForeTiS) (0.6.0)\n",
      "Requirement already satisfied: deprecated in /home/josef/.local/lib/python3.10/site-packages (from gpflow==2.5.2->ForeTiS) (1.2.13)\n",
      "Requirement already satisfied: numpy in /home/josef/.local/lib/python3.10/site-packages (from gpflow==2.5.2->ForeTiS) (1.22.2)\n",
      "Requirement already satisfied: tqdm in /home/josef/.local/lib/python3.10/site-packages (from optuna==3.0.2->ForeTiS) (4.64.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/lib/python3/dist-packages (from optuna==3.0.2->ForeTiS) (1.4.31)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /home/josef/.local/lib/python3.10/site-packages (from optuna==3.0.2->ForeTiS) (0.8.2)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from optuna==3.0.2->ForeTiS) (5.4.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/josef/.local/lib/python3.10/site-packages (from optuna==3.0.2->ForeTiS) (1.8.1)\n",
      "Requirement already satisfied: cliff in /home/josef/.local/lib/python3.10/site-packages (from optuna==3.0.2->ForeTiS) (4.0.0)\n",
      "Requirement already satisfied: colorlog in /home/josef/.local/lib/python3.10/site-packages (from optuna==3.0.2->ForeTiS) (6.7.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas==1.5.0->ForeTiS) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/lib/python3/dist-packages (from pandas==1.5.0->ForeTiS) (2.8.1)\n",
      "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /home/josef/.local/lib/python3.10/site-packages (from pmdarima==2.0.1->ForeTiS) (0.29.32)\n",
      "Requirement already satisfied: urllib3 in /usr/lib/python3/dist-packages (from pmdarima==2.0.1->ForeTiS) (1.26.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/josef/.local/lib/python3.10/site-packages (from scikit-learn==1.1.2->ForeTiS) (3.1.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /home/josef/.local/lib/python3.10/site-packages (from statsmodels==0.13.2->ForeTiS) (0.5.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow==2.10.0->ForeTiS) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.10.0->ForeTiS) (1.16.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow==2.10.0->ForeTiS) (0.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow==2.10.0->ForeTiS) (1.14.1)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow==2.10.0->ForeTiS) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow==2.10.0->ForeTiS) (2.0.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow==2.10.0->ForeTiS) (2.10.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow==2.10.0->ForeTiS) (1.48.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow==2.10.0->ForeTiS) (3.3.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow==2.10.0->ForeTiS) (2.10.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow==2.10.0->ForeTiS) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow==2.10.0->ForeTiS) (1.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow==2.10.0->ForeTiS) (0.27.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow==2.10.0->ForeTiS) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow==2.10.0->ForeTiS) (3.7.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/lib/python3/dist-packages (from tensorflow==2.10.0->ForeTiS) (3.12.4)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow==2.10.0->ForeTiS) (2.0.7)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow==2.10.0->ForeTiS) (14.0.6)\n",
      "Requirement already satisfied: decorator in /home/josef/.local/lib/python3.10/site-packages (from tensorflow-probability==0.18->ForeTiS) (5.1.1)\n",
      "Requirement already satisfied: dm-tree in /home/josef/.local/lib/python3.10/site-packages (from tensorflow-probability==0.18->ForeTiS) (0.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /home/josef/.local/lib/python3.10/site-packages (from tensorflow-probability==0.18->ForeTiS) (2.2.0)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in /home/josef/.local/lib/python3.10/site-packages (from bayesian-torch->ForeTiS) (0.13.1)\n",
      "Requirement already satisfied: pillow>=7.1 in /usr/lib/python3/dist-packages (from blitz-bayesian-pytorch->ForeTiS) (9.0.1)\n",
      "Requirement already satisfied: nose in /home/josef/.local/lib/python3.10/site-packages (from changefinder->ForeTiS) (1.3.7)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/lib/python3/dist-packages (from matplotlib->ForeTiS) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib->ForeTiS) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib/python3/dist-packages (from matplotlib->ForeTiS) (1.3.2)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna==3.0.2->ForeTiS) (1.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow==2.10.0->ForeTiS) (0.37.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/lib/python3/dist-packages (from sqlalchemy>=1.3.0->optuna==3.0.2->ForeTiS) (1.1.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/josef/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0->ForeTiS) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/josef/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0->ForeTiS) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/josef/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0->ForeTiS) (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/josef/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0->ForeTiS) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/josef/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0->ForeTiS) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0->ForeTiS) (2.25.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/josef/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0->ForeTiS) (2.11.1)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /home/josef/.local/lib/python3.10/site-packages (from cliff->optuna==3.0.2->ForeTiS) (4.0.0)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /home/josef/.local/lib/python3.10/site-packages (from cliff->optuna==3.0.2->ForeTiS) (3.4.1)\n",
      "Requirement already satisfied: autopage>=0.4.0 in /home/josef/.local/lib/python3.10/site-packages (from cliff->optuna==3.0.2->ForeTiS) (0.5.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/lib/python3/dist-packages (from cliff->optuna==3.0.2->ForeTiS) (4.6.4)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /home/josef/.local/lib/python3.10/site-packages (from cliff->optuna==3.0.2->ForeTiS) (2.4.2)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /home/josef/.local/lib/python3.10/site-packages (from cmd2>=1.0.0->cliff->optuna==3.0.2->ForeTiS) (0.2.5)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /home/josef/.local/lib/python3.10/site-packages (from cmd2>=1.0.0->cliff->optuna==3.0.2->ForeTiS) (1.8.2)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /home/josef/.local/lib/python3.10/site-packages (from cmd2>=1.0.0->cliff->optuna==3.0.2->ForeTiS) (22.1.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0->ForeTiS) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0->ForeTiS) (0.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0->ForeTiS) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/lib/python3/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0->ForeTiS) (1.3.0)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /home/josef/.local/lib/python3.10/site-packages (from stevedore>=2.0.1->cliff->optuna==3.0.2->ForeTiS) (5.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/josef/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0->ForeTiS) (2.1.1)\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -i https://test.pypi.org/simple/ ForeTiS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ForeTiS\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/josef/Schreibtisch/01_HorticulturalSalesPrediction/ForeTiS/docs/source/tutorials\n"
     ]
    }
   ],
   "source": [
    "# Definition of paths and filenames\n",
    "cwd = pathlib.Path.cwd()\n",
    "data_dir = cwd.joinpath('tutorial_data')\n",
    "data = 'nike_sales'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run whole optimization pipeline at once\n",
    "As shown for the [Docker workflow](https://ForeTiS.readthedocs.io/en/latest/tutorials/tut_run_docker.html), ForeTiS offers a function [optim_pipeline.run()](https://github.com/grimmlab/ForeTiS/blob/master/ForeTiS/optim_pipeline.py) that triggers the whole optimization run. \n",
    "\n",
    "In the definition of ``optim_pipeline.run()``, we set several default values. In order to run it using our tutorial data, we just need to define the data and directories we want to use as well as the models we want to optimize. Furthermore, we set values for the ``datasplit`` and ``n_trials`` to limit the waiting time for getting the results.\n",
    "\n",
    "When calling the function, we first see some information regarding the data preprocessing and the configuration of our optimization run, e.g. the data that is used. Then, the current progress of the optuna optimization with results of the individual trials is shown. In the end, we show a summary of the whole optimization run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'nike_sales'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_44731/2245082882.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m ForeTiS.optim_pipeline.run(\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0mdata_dir\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig_file\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'nike_sales'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msave_dir\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'xgboost'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_trials\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mperiodical_refit_cycles\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'complete'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent_lags\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m7\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m )\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/ForeTiS/optim_pipeline.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(data_dir, save_dir, datasplit, test_set_size_percentage, val_set_size_percentage, imputation_method, windowsize_current_statistics, windowsize_lagged_statistics, models, n_trials, pca_transform, save_final_model, periodical_refit_cycles, refit_drops, data, config_file, refit_window, intermediate_results_interval, batch_size, n_epochs, event_lags, optimize_featureset, scale_thr, scale_seasons, cf_thr_perc, scale_window_factor, cf_r, cf_order, cf_smooth, scale_window_minimum, max_samples_factor, valtest_seasons, seasonal_valtest)\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[0mconfig\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconfigparser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mConfigParser\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mallow_no_value\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Config/dataset_specific_config.ini'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m     datasets = base_dataset.Dataset(data_dir=data_dir, data=data, config_file=config_file, event_lags=event_lags,\n\u001B[0m\u001B[1;32m     28\u001B[0m                                     \u001B[0mtest_set_size_percentage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtest_set_size_percentage\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m                                     \u001B[0mwindowsize_current_statistics\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mwindowsize_current_statistics\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/ForeTiS/preprocess/base_dataset.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data_dir, data, config_file, test_set_size_percentage, windowsize_current_statistics, windowsize_lagged_statistics, imputation_method, config, event_lags, valtest_seasons, seasonal_valtest)\u001B[0m\n\u001B[1;32m     55\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muser_input_params\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlocals\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# distribute all handed over params in whole class\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 57\u001B[0;31m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mconfig_file\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     58\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues_for_counter\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mconfig_file\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'values_for_counter'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreplace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\" \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m','\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;34m''\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues_for_counter\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.10/configparser.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    962\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    963\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdefault_section\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhas_section\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 964\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    965\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_proxies\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    966\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'nike_sales'"
     ]
    }
   ],
   "source": [
    "ForeTiS.optim_pipeline.run(\n",
    "    data_dir=data_dir, data=data, config_file='nike_sales', save_dir=data_dir, models=['xgboost'], n_trials=10, periodical_refit_cycles=['complete', 0, 1, 2], event_lags=[-7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the defined ``save_dir``, a ``results`` folder will be created. \n",
    "   \n",
    "Then, ForeTiS' default folder structure follows: ``model/featureset/``.\n",
    "\n",
    "We can see this structure below with all optimization results for the defined ``phenotype``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folders = list(save_dir.joinpath('results').glob('*'))\n",
    "for results_dir in result_folders:\n",
    "    print(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we can see that each result folder contains a ``Results_overview_*.csv`` as well as detailed results for each of the optimized models. In case of ``nested-cv``, this is preceded by a subfolder for each of the outer folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/fhaselbeck/PycharmProjects/results/x_matrix/y_matrix/continuous_values/cv-test_5-20_MAF0_xgboost_2022-10-04_16-07-04/xgboost\n",
      "/home/fhaselbeck/PycharmProjects/results/x_matrix/y_matrix/continuous_values/cv-test_5-20_MAF0_xgboost_2022-10-04_16-07-04/Results_overview_xgboost.csv\n"
     ]
    }
   ],
   "source": [
    "result_elements = list(result_folders[0].glob('*'))\n",
    "for result_element in result_elements:\n",
    "    print(result_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Results_overview_*.csv`` file contains the best parameters, evaluation as well as runtime metrics for each of the optimized models as we can see in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>xgboost___best_params</th>\n",
       "      <th>xgboost___eval_metrics</th>\n",
       "      <th>xgboost___runtime_metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>[{'colsample_bytree': 0.05, 'gamma': 520, 'lea...</td>\n",
       "      <td>[{'test_mse': 393.8179197745938, 'test_rmse': ...</td>\n",
       "      <td>[{'process_time_mean': 20.092465339700002, 'pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                              xgboost___best_params  \\\n",
       "0       Test  [{'colsample_bytree': 0.05, 'gamma': 520, 'lea...   \n",
       "\n",
       "                              xgboost___eval_metrics  \\\n",
       "0  [{'test_mse': 393.8179197745938, 'test_rmse': ...   \n",
       "\n",
       "                           xgboost___runtime_metrics  \n",
       "0  [{'process_time_mean': 20.092465339700002, 'pr...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_overview_file = [overview_file for overview_file in result_elements if 'Results_overview' in str(overview_file)][0]\n",
    "pd.read_csv(results_overview_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond that, we see below that the detailed results for each optimized model contain validation and test results, saved prediction models, an optuna database, a runtime overview with information for each trial (good for debugging, as pruning reasons are also documented) and for some prediction models also feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/fhaselbeck/PycharmProjects/results/x_matrix/y_matrix/continuous_values/cv-test_5-20_MAF0_xgboost_2022-10-04_16-07-04/xgboost/Optuna_DB.db\n",
      "/home/fhaselbeck/PycharmProjects/results/x_matrix/y_matrix/continuous_values/cv-test_5-20_MAF0_xgboost_2022-10-04_16-07-04/xgboost/validation_results_trial4.csv\n",
      "/home/fhaselbeck/PycharmProjects/results/x_matrix/y_matrix/continuous_values/cv-test_5-20_MAF0_xgboost_2022-10-04_16-07-04/xgboost/xgboost_runtime_overview.csv\n",
      "/home/fhaselbeck/PycharmProjects/results/x_matrix/y_matrix/continuous_values/cv-test_5-20_MAF0_xgboost_2022-10-04_16-07-04/xgboost/unfitted_model_trial4\n",
      "/home/fhaselbeck/PycharmProjects/results/x_matrix/y_matrix/continuous_values/cv-test_5-20_MAF0_xgboost_2022-10-04_16-07-04/xgboost/final_model_test_results.csv\n",
      "/home/fhaselbeck/PycharmProjects/results/x_matrix/y_matrix/continuous_values/cv-test_5-20_MAF0_xgboost_2022-10-04_16-07-04/xgboost/final_model_feature_importances.csv\n"
     ]
    }
   ],
   "source": [
    "for subdir in [overview_file for overview_file in result_elements if 'Results_overview' not in str(overview_file)][0].rglob('*'):\n",
    "    print(subdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single elements of the optimization pipeline\n",
    "For a better understanding of the whole optimization pipeline, we subsequently show some of the single elements which are called within ``optim_pipeline.run()``. \n",
    "\n",
    "First, ``optim_pipeline.run()`` contains some functions to check the specified arguments, which we will skip for this tutorial. However, we need to define some of the default values and create ``pathlib.Path`` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(data_dir)\n",
    "save_dir = pathlib.Path(save_dir)\n",
    "datasplit = 'cv-test'\n",
    "n_innerfolds = 5\n",
    "test_set_size_percentage=20\n",
    "maf_percentage = 0\n",
    "models = ['xgboost']\n",
    "n_trials = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of the optimization pipeline is the preparation of the raw data files using ``easypheno.preprocess.raw_data_functions.prepare_data_files()``. If the format matches our [Data Guide](https://easypheno.readthedocs.io/en/latest/data.html#), the raw data files are preprocessed.\n",
    "\n",
    "The genotype matrix is converted and unified to a ``.h5`` file and saved with the same name as the raw file, if this genotype matrix is used for the first time. \n",
    "\n",
    "The phenotype matrix is checked whether the format is fine, but not saved in a different format.\n",
    "\n",
    "An index file containing indices for filtering the data (e.g. maf or duplicates) and creating the data splits is saved or updated in case it already exists and a datasplit that is currently not present in the file is requested. This ensures reproducibility of the preprocessing and data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if all data files have the required format\n",
      "Found same file name with ending .h5\n",
      "Assuming that the raw file was already prepared using our pipepline. Will continue with the .h5 file.\n",
      "Genotype file available in required format, check index file now.\n",
      "Index file x_matrix-y_matrix-continuous_values.h5 already exists. Will append required filters and data splits now.\n",
      "Done checking data files. All required datasets are available.\n"
     ]
    }
   ],
   "source": [
    "easypheno.preprocess.raw_data_functions.prepare_data_files(\n",
    "    data_dir=data_dir, genotype_matrix_name=genotype_matrix, phenotype_matrix_name=phenotype_matrix,\n",
    "    phenotype=phenotype, datasplit=datasplit, n_outerfolds=5, n_innerfolds=n_innerfolds,\n",
    "    test_set_size_percentage=test_set_size_percentage, val_set_size_percentage=20,\n",
    "    models=models, user_encoding=None, maf_percentage=maf_percentage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting all seeds for reproducibility using ``easypheno.utils.helper_functions.set_all_seeds()``, a model for the current optimization is selected. This information is then used to retrieve its ``standard_encoding`` if the user did not define an encoding. \n",
    "\n",
    "With this information, the ``easypheno.preprocess.base_dataset.Dataset`` object is initialized.  \n",
    "We also print some information regarding the current progress, as loading the data might take some time for bigger datasets.   \n",
    "When running the optimization for multiple models, these are sorted according to their encoding and the dataset is only loaded new if the encoding changes between models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and match raw data\n",
      "Apply MAF filter\n",
      "Filter duplicate SNPs\n",
      "Check if final snp_ids already exist in index_file for used encoding and maf percentage. Save them if necessary.\n",
      "Load datasplit file\n",
      "Checked datasplit for all folds.\n"
     ]
    }
   ],
   "source": [
    "easypheno.utils.helper_functions.set_all_seeds()\n",
    "current_model_name = models[0]\n",
    "encoding = easypheno.utils.helper_functions.get_mapping_name_to_class()[current_model_name].standard_encoding\n",
    "\n",
    "dataset = easypheno.preprocess.base_dataset.Dataset(\n",
    "    data_dir=data_dir, genotype_matrix_name=genotype_matrix, phenotype_matrix_name=phenotype_matrix,\n",
    "    phenotype=phenotype, datasplit=datasplit, n_outerfolds=5, n_innerfolds=n_innerfolds,\n",
    "    test_set_size_percentage=test_set_size_percentage, val_set_size_percentage=20,\n",
    "    encoding=encoding, maf_percentage=maf_percentage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After retrieving the type of ML ``task`` using ``easypheno.utils.helper_functions.test_likely_categorical()`` as well as the time stamp for saving the results, we create an ``easypheno.optimization.optuna_optim.OptunaOptim`` object. For this purpose, we handover all information that is needed for the hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'classification' if easypheno.utils.helper_functions.test_likely_categorical(dataset.y_full) else 'regression'\n",
    "models_start_time = '+'.join(models) + '_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "optim_run = easypheno.optimization.optuna_optim.OptunaOptim(\n",
    "    save_dir=save_dir, genotype_matrix_name=genotype_matrix, phenotype_matrix_name=phenotype_matrix,\n",
    "    phenotype=phenotype, n_outerfolds=5, n_innerfolds=n_innerfolds,val_set_size_percentage=20, \n",
    "    test_set_size_percentage=test_set_size_percentage, maf_percentage=maf_percentage, n_trials=n_trials, \n",
    "    save_final_model=False, batch_size=None, n_epochs=10000, task=task, \n",
    "    models_start_time=models_start_time, current_model_name=current_model_name, dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we just need to call the method ``run()`` of our ``easypheno.optimization.optuna_optim.OptunaOptim`` object to start the Bayesian hyperparameter search, which will print the current progress and return a dictionary with summary results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-10-04 16:08:39,294]\u001B[0m A new study created in RDB with name: 2022-10-04_16-08-33_x_matrix-y_matrix-continuous_values-MAF0-SPLITcv-test5-20-MODELxgboost-TRIALS10\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params for Trial 0\n",
      "{'n_estimators': 2500, 'learning_rate': 0.07500000000000001, 'max_depth': 3, 'gamma': 300, 'subsample': 0.45, 'colsample_bytree': 0.35000000000000003, 'reg_alpha': 290.0}\n",
      "# Processing innerfold_0 #\n",
      "# Processing innerfold_1 #\n",
      "# Processing innerfold_2 #\n",
      "# Processing innerfold_3 #\n",
      "# Processing innerfold_4 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-10-04 16:08:46,932]\u001B[0m Trial 0 finished with value: 369.22728277444065 and parameters: {'n_estimators': 2500, 'learning_rate': 0.07500000000000001, 'max_depth': 3, 'gamma': 300, 'subsample': 0.45, 'colsample_bytree': 0.35000000000000003, 'reg_alpha': 290.0}. Best is trial 0 with value: 369.22728277444065.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params for Trial 1\n",
      "{'n_estimators': 3000, 'learning_rate': 0.3, 'max_depth': 9, 'gamma': 300, 'subsample': 0.1, 'colsample_bytree': 0.55, 'reg_alpha': 440.0}\n",
      "# Processing innerfold_0 #\n",
      "# Processing innerfold_1 #\n",
      "# Processing innerfold_2 #\n",
      "# Processing innerfold_3 #\n",
      "# Processing innerfold_4 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-10-04 16:08:59,323]\u001B[0m Trial 1 finished with value: 454.8753047090302 and parameters: {'n_estimators': 3000, 'learning_rate': 0.3, 'max_depth': 9, 'gamma': 300, 'subsample': 0.1, 'colsample_bytree': 0.55, 'reg_alpha': 440.0}. Best is trial 0 with value: 369.22728277444065.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params for Trial 2\n",
      "{'n_estimators': 2250, 'learning_rate': 0.2, 'max_depth': 10, 'gamma': 80, 'subsample': 0.2, 'colsample_bytree': 0.05, 'reg_alpha': 320.0}\n",
      "# Processing innerfold_0 #\n",
      "# Processing innerfold_1 #\n",
      "# Processing innerfold_2 #\n",
      "# Processing innerfold_3 #\n",
      "# Processing innerfold_4 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-10-04 16:09:11,364]\u001B[0m Trial 2 finished with value: 398.20160514328586 and parameters: {'n_estimators': 2250, 'learning_rate': 0.2, 'max_depth': 10, 'gamma': 80, 'subsample': 0.2, 'colsample_bytree': 0.05, 'reg_alpha': 320.0}. Best is trial 0 with value: 369.22728277444065.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params for Trial 3\n",
      "{'n_estimators': 2000, 'learning_rate': 0.225, 'max_depth': 8, 'gamma': 770, 'subsample': 0.1, 'colsample_bytree': 0.3, 'reg_alpha': 110.0}\n",
      "# Processing innerfold_0 #\n",
      "# Processing innerfold_1 #\n",
      "# Processing innerfold_2 #\n",
      "# Processing innerfold_3 #\n",
      "# Processing innerfold_4 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-10-04 16:09:24,604]\u001B[0m Trial 3 finished with value: 369.6724074334373 and parameters: {'n_estimators': 2000, 'learning_rate': 0.225, 'max_depth': 8, 'gamma': 770, 'subsample': 0.1, 'colsample_bytree': 0.3, 'reg_alpha': 110.0}. Best is trial 0 with value: 369.22728277444065.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params for Trial 4\n",
      "{'n_estimators': 1750, 'learning_rate': 0.25, 'max_depth': 6, 'gamma': 520, 'subsample': 0.35000000000000003, 'colsample_bytree': 0.05, 'reg_alpha': 100.0}\n",
      "# Processing innerfold_0 #\n",
      "# Processing innerfold_1 #\n",
      "# Processing innerfold_2 #\n",
      "# Processing innerfold_3 #\n",
      "# Processing innerfold_4 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-10-04 16:09:31,163]\u001B[0m Trial 4 finished with value: 364.574017351775 and parameters: {'n_estimators': 1750, 'learning_rate': 0.25, 'max_depth': 6, 'gamma': 520, 'subsample': 0.35000000000000003, 'colsample_bytree': 0.05, 'reg_alpha': 100.0}. Best is trial 4 with value: 364.574017351775.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params for Trial 5\n",
      "{'n_estimators': 2750, 'learning_rate': 0.2, 'max_depth': 9, 'gamma': 810, 'subsample': 0.15000000000000002, 'colsample_bytree': 0.7500000000000001, 'reg_alpha': 540.0}\n",
      "# Processing innerfold_0 #\n",
      "# Processing innerfold_1 #\n",
      "# Processing innerfold_2 #\n",
      "# Processing innerfold_3 #\n",
      "# Processing innerfold_4 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-10-04 16:09:40,337]\u001B[0m Trial 5 finished with value: 452.7427816744316 and parameters: {'n_estimators': 2750, 'learning_rate': 0.2, 'max_depth': 9, 'gamma': 810, 'subsample': 0.15000000000000002, 'colsample_bytree': 0.7500000000000001, 'reg_alpha': 540.0}. Best is trial 4 with value: 364.574017351775.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params for Trial 6\n",
      "{'n_estimators': 100, 'learning_rate': 0.3, 'max_depth': 4, 'gamma': 520, 'subsample': 0.6000000000000001, 'colsample_bytree': 0.3, 'reg_alpha': 980.0}\n",
      "# Processing innerfold_0 #\n",
      "# Processing innerfold_1 #\n",
      "# Processing innerfold_2 #\n",
      "# Processing innerfold_3 #\n",
      "# Processing innerfold_4 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-10-04 16:09:46,464]\u001B[0m Trial 6 finished with value: 447.1631958152293 and parameters: {'n_estimators': 100, 'learning_rate': 0.3, 'max_depth': 4, 'gamma': 520, 'subsample': 0.6000000000000001, 'colsample_bytree': 0.3, 'reg_alpha': 980.0}. Best is trial 4 with value: 364.574017351775.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params for Trial 7\n",
      "{'n_estimators': 50, 'learning_rate': 0.3, 'max_depth': 4, 'gamma': 670, 'subsample': 0.6500000000000001, 'colsample_bytree': 0.2, 'reg_alpha': 730.0}\n",
      "# Processing innerfold_0 #\n",
      "# Processing innerfold_1 #\n",
      "# Processing innerfold_2 #\n",
      "# Processing innerfold_3 #\n",
      "# Processing innerfold_4 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-10-04 16:09:49,729]\u001B[0m Trial 7 finished with value: 420.049507545245 and parameters: {'n_estimators': 50, 'learning_rate': 0.3, 'max_depth': 4, 'gamma': 670, 'subsample': 0.6500000000000001, 'colsample_bytree': 0.2, 'reg_alpha': 730.0}. Best is trial 4 with value: 364.574017351775.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params for Trial 8\n",
      "{'n_estimators': 1000, 'learning_rate': 0.2, 'max_depth': 3, 'gamma': 690, 'subsample': 0.35000000000000003, 'colsample_bytree': 0.7500000000000001, 'reg_alpha': 130.0}\n",
      "# Processing innerfold_0 #\n",
      "# Processing innerfold_1 #\n",
      "# Processing innerfold_2 #\n",
      "# Processing innerfold_3 #\n",
      "# Processing innerfold_4 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-10-04 16:09:56,208]\u001B[0m Trial 8 finished with value: 373.9128450040709 and parameters: {'n_estimators': 1000, 'learning_rate': 0.2, 'max_depth': 3, 'gamma': 690, 'subsample': 0.35000000000000003, 'colsample_bytree': 0.7500000000000001, 'reg_alpha': 130.0}. Best is trial 4 with value: 364.574017351775.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params for Trial 9\n",
      "{'n_estimators': 250, 'learning_rate': 0.125, 'max_depth': 5, 'gamma': 730, 'subsample': 0.7500000000000001, 'colsample_bytree': 0.7500000000000001, 'reg_alpha': 780.0}\n",
      "# Processing innerfold_0 #\n",
      "# Processing innerfold_1 #\n",
      "# Processing innerfold_2 #\n",
      "# Processing innerfold_3 #\n",
      "# Processing innerfold_4 #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-10-04 16:10:00,982]\u001B[0m Trial 9 finished with value: 419.9258576836199 and parameters: {'n_estimators': 250, 'learning_rate': 0.125, 'max_depth': 5, 'gamma': 730, 'subsample': 0.7500000000000001, 'colsample_bytree': 0.7500000000000001, 'reg_alpha': 780.0}. Best is trial 4 with value: 364.574017351775.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Optuna Study finished ##\n",
      "Study statistics: \n",
      "  Finished trials:  10\n",
      "  Pruned trials:  0\n",
      "  Completed trials:  10\n",
      "  Best Trial:  4\n",
      "  Value:  364.574017351775\n",
      "  Params: \n",
      "    colsample_bytree: 0.05\n",
      "    gamma: 520\n",
      "    learning_rate: 0.25\n",
      "    max_depth: 6\n",
      "    n_estimators: 1750\n",
      "    reg_alpha: 100.0\n",
      "    subsample: 0.35000000000000003\n",
      "## Retrain best model and test ##\n",
      "## Results on test set ##\n",
      "{'test_mse': 393.8179197745938, 'test_rmse': 19.84484617664228, 'test_r2_score': 0.05719242798046553, 'test_explained_variance': 0.05724940832753167}\n",
      "{'Test': {'best_params': {'colsample_bytree': 0.05,\n",
      "                          'gamma': 520,\n",
      "                          'learning_rate': 0.25,\n",
      "                          'max_depth': 6,\n",
      "                          'n_estimators': 1750,\n",
      "                          'reg_alpha': 100.0,\n",
      "                          'subsample': 0.35000000000000003},\n",
      "          'eval_metrics': {'test_explained_variance': 0.05724940832753167,\n",
      "                           'test_mse': 393.8179197745938,\n",
      "                           'test_r2_score': 0.05719242798046553,\n",
      "                           'test_rmse': 19.84484617664228},\n",
      "          'runtime_metrics': {'process_time_max': 36.19791426699999,\n",
      "                              'process_time_mean': 20.51563152270001,\n",
      "                              'process_time_min': 1.4952742340000214,\n",
      "                              'process_time_std': 13.181602909370394,\n",
      "                              'real_time_max': 11.987335681915283,\n",
      "                              'real_time_mean': 7.454001760482788,\n",
      "                              'real_time_min': 3.102874517440796,\n",
      "                              'real_time_std': 3.14674639916681}}}\n"
     ]
    }
   ],
   "source": [
    "summary_results = optim_run.run_optuna_optimization()\n",
    "pprint.PrettyPrinter(depth=4).pprint(summary_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond that, ``easypheno.optimization.optuna_optim.OptunaOptim`` creates and saves the ``Results_overview_*.csv`` files, which we show above in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further information\n",
    "This notebooks shows how the use the easyPheno pip package to run an optimization. Furthermore, we give an overview of the individual steps within ``optim_pipeline.run()``.\n",
    "\n",
    "For more information on specific topcis, see the following links:\n",
    "\n",
    "- [Documentation of the whole package](https://easypheno.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "- [easyPheno's GitHub repository](https://github.com/grimmlab/easyPheno)\n",
    "\n",
    "- Prepare your data according to our format: [Data Guide](https://easypheno.readthedocs.io/en/latest/data.html)\n",
    "\n",
    "- The [Installation Guide](https://easypheno.readthedocs.io/en/latest/install_docker.html) as well as [basic tutorial](https://easypheno.readthedocs.io/en/latest/tutorials/tut_run_docker.html) for the Docker workflow as an alternative\n",
    "\n",
    "- Summarize and analyze prediction results with easyPeno: [HowTo: Summarize prediction results with easyPheno](https://easypheno.readthedocs.io/en/latest/tutorials/tut_sum_results.html)\n",
    "\n",
    "- Several [advanced topics](https://easypheno.readthedocs.io/en/latest/tutorials/tut_adv.html) such as adjusting existing prediction models or creation of new ones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
